{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earth Engine package initialized successfully..\n",
      "config loaded.\n"
     ]
    },
    {
     "ename": "EEException",
     "evalue": "User memory limit exceeded.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEEException\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-71d2dac99eb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;31m#band_month = strptime(run_date,'%b').tm_mon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0ms_bm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mband_month\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m \u001b[0mdff_wc_rename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_wc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;31m#dff_ecmwf_rename = get_ecmwf()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0mdff_sm2_rename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sm2map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-71d2dac99eb3>\u001b[0m in \u001b[0;36mget_wc\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mwc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mee\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'users/wfphqgis/CLIM/wc20_30s_prec_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0ms_bm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mwc_vector_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspatial_red\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mee\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReducer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mdf_wc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meeconvert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfcToGdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwc_vector_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0mdff_wc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_wc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'adm0_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'adm1_name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'adm2_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'adm0_code'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'adm1_code'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'disp_area'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'geometry'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'status'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0mdff_wc_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdff_wc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'mode'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'WC_Mean_Prec'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/adamf/lib/python3.7/site-packages/eeconvert/__init__.py\u001b[0m in \u001b[0;36mfcToGdf\u001b[0;34m(fc, crs)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mcrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'init'\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m'epsg:4326'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mdictarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/adamf/lib/python3.7/site-packages/ee/collection.py\u001b[0m in \u001b[0;36mgetInfo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m            \u001b[0mproperties\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \"\"\"\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCollection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_property\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_ascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/adamf/lib/python3.7/site-packages/ee/computedobject.py\u001b[0m in \u001b[0;36mgetInfo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m       \u001b[0mThe\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mevaluate\u001b[0m \u001b[0mto\u001b[0m \u001b[0manything\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \"\"\"\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'json'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/adamf/lib/python3.7/site-packages/ee/data.py\u001b[0m in \u001b[0;36mgetValue\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    267\u001b[0m   \"\"\"\n\u001b[1;32m    268\u001b[0m   \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'json_format'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'v2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msend_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/value'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/adamf/lib/python3.7/site-packages/ee/data.py\u001b[0m in \u001b[0;36msend_\u001b[0;34m(path, params, opt_method, opt_raw)\u001b[0m\n\u001b[1;32m    826\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mee_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEEException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid JSON: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson_content\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mee_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEEException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_content\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'data'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mee_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEEException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Malformed response: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEEException\u001b[0m: User memory limit exceeded."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Extreme Rainfall Detection and Alerting System (ERDAS) v 0.1\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "__author__ = 'OSE GIS'\n",
    "__project__ = 'Extreme Rainfall Detection and Alerting System (ERDAS)'\n",
    "__contact__ = 'michael.manalili@wfp.org', 'wfp.hq.gis@wfp.org'\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "import requests, os, time, shutil\n",
    "from time import strptime\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta, date\n",
    "import pandas as pd\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import descartes\n",
    "import schedule\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart #python3\n",
    "from email.mime.base import MIMEBase #python3\n",
    "from email import encoders #python3\n",
    "from sentinelsat.sentinel import SentinelAPI, read_geojson, geojson_to_wkt\n",
    "import ee\n",
    "import eeconvert\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from rasterstats import zonal_stats\n",
    "import cdsapi\n",
    "from geopandas import GeoSeries, GeoDataFrame\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2 \n",
    "import io\n",
    "from lxml import html\n",
    "import wget\n",
    "import cdsapi\n",
    "from configparser import ConfigParser\n",
    "import sentinelsat\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2 \n",
    "import pandas.io.sql as psql\n",
    "from io import BytesIO as StringIO\n",
    "from osgeo import ogr\n",
    "from ftplib import FTP\n",
    "import pycountry\n",
    "import uuid\n",
    "#import pysftp\n",
    "#from io import BytesIO as StringIO\n",
    "\n",
    "try:\n",
    "    ee.Initialize()\n",
    "    print ('Earth Engine package initialized successfully..')\n",
    "except ee.EEException as e:\n",
    "    print ('Earth Engine package failed to initialize!')\n",
    "except:\n",
    "    print ('Unexpected error:', sys.exc_info()[0])\n",
    "    raise\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "#pd.set_option('mode.chained_assignment', 'raise')\n",
    "\n",
    "execTime = time.time()\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "#import geopandas as gpd\n",
    "#dir(gpd.GeoDataFrame)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "now = datetime.today()\n",
    "d = str(now)\n",
    "date = d[0:10]\n",
    "date_cut = date.replace('-', '')\n",
    "whitelist_data = 'extreme_precip_alert_' + date_cut\n",
    "alert_data = 'red_alert_' + date_cut\n",
    "\n",
    "script = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "root = os.path.dirname(script)\n",
    "\n",
    "# Assumed this folders are pre-created\n",
    "process_root = root + '/processing'\n",
    "process_amsr2 = process_root + '/process_amsr2/'\n",
    "process_cdsapi = process_root + '/process_cdsapi/'\n",
    "process_ecmwf = process_root + '/process_ecmwf/'\n",
    "process_pxr = process_root + '/process_pxr/'\n",
    "\n",
    "alert_csv_path = root + '/alert/'\n",
    "s1_txt_path = root + '/alert/'\n",
    "map_view_path = root + '/view/'\n",
    "\n",
    "alert_data_name = 'Extreme_rainfall_alert_beta_' + date_cut + '.csv'\n",
    "map_view_name = 'Map_view_' + date_cut + '.html'\n",
    "s1_name = 'sentinel_sched_' + date_cut + '.txt'\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "config = ConfigParser()\n",
    "config.read(os.path.join(root, \"config.txt\"))\n",
    "\n",
    "esa_un = config.get('sentinelsat', 'ss_un')\n",
    "esa_pw = config.get('sentinelsat', 'ss_pw')\n",
    "esa_link = config.get('sentinelsat', 'ss_link')\n",
    "api = SentinelAPI(esa_un, esa_pw, esa_link)\n",
    "api\n",
    "\n",
    "config.read(os.path.join(root, \"config.txt\"))\n",
    "smtp_un = config.get('dbtrack','dbt_uname')\n",
    "smtp_pw = config.get('dbtrack','dbt_pword')\n",
    "print('config loaded.')\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "#ECMWF_SDI_raster_cat = config.get(\"ECMWF\",\"ECMWF_SDI_raster_cat\")\n",
    "#ECMWF_SDI_raster_cat = os.path.join(connfiles_folder,ECMWF_SDI_raster_cat)\n",
    "\n",
    "#ECMWF_SDI_mosaic_dataset= config.get(\"ECMWF\",\"ECMWF_SDI_mosaic_dataset\")\n",
    "#ECMWF_SDI_mosaic_dataset = os.path.join(connfiles_folder,ECMWF_SDI_mosaic_dataset)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def spatial_red(ans, reducer, scale):\n",
    "    data = ans.reduceRegions(\n",
    "            collection = wfp_aoi,\n",
    "            reducer = reducer,\n",
    "            crs='EPSG:3857',\n",
    "            scale=scale,\n",
    "            tileScale=1)\n",
    "    return data   \n",
    "\n",
    "def write2file(filename, header, gdf):\n",
    "    data = gdf.to_csv(filename, columns=header, encoding='utf-8')\n",
    "    return data\n",
    "\n",
    "def export2drive_raster(raster, bbox ,prefix,scale):\n",
    "    run = ee.batch.Export.image.toDrive(\n",
    "         image=raster, \n",
    "         scale=scale,\n",
    "         fileNamePrefix = prefix,\n",
    "         region = ee.Feature(bbox).geometry().bounds().getInfo()['coordinates'], #can also be single country\n",
    "         maxPixels = 1e13,\n",
    "         fileFormat='GeoTiff',\n",
    "         cloudOptimized=True).start()\n",
    "    return run\n",
    "\n",
    "#Format Options: GeoJSON, SHP, KML\n",
    "def export2drive_table(ee_featCol, description):\n",
    "    table = ee.batch.Export.table.toDrive(\n",
    "         collection=ee_featCol, \n",
    "         description=description,\n",
    "         fileFormat='CSV')#.start() \n",
    "    return table\n",
    "\n",
    "def month(n):\n",
    "    start = ee.Date('2014-01-01').advance(n, 'month')\n",
    "    end = start.advance(1,'month')\n",
    "    return ee.ImageCollection('JAXA/GPM_L3/GSMaP/v6/operational').select('hourlyPrecipRate').filterDate(start,end).mean().set('system:time_start', start.millis())\n",
    "\n",
    "def get_wc():\n",
    "    #x = ee.Image(collectionList.get(band_month))\n",
    "    wc = ee.Image('users/wfphqgis/CLIM/wc20_30s_prec_'+s_bm)\n",
    "    wc_vector_mean = spatial_red(wc,ee.Reducer.mode(),900)\n",
    "    df_wc = eeconvert.fcToGdf(wc_vector_mean)\n",
    "    dff_wc = df_wc.drop(columns = ['adm0_name', 'adm1_name','adm2_name', 'adm0_code', 'adm1_code','disp_area','geometry','status'])\n",
    "    dff_wc_r = dff_wc.rename(columns={'mode':'WC_Mean_Prec'})\n",
    "    return dff_wc_r\n",
    "\n",
    "def get_sm2map():\n",
    "    #x = ee.Image(collectionList.get(band_month))\n",
    "    sm2 = ee.Image('users/wfphqgis/CLIM/clm_precipitation_sm2rain_m_1km_s00cm_20072018_v02_'+s_bm)\n",
    "    sm2_vector_mean = spatial_red(sm2,ee.Reducer.mode(),900)\n",
    "    df_sm2 = eeconvert.fcToGdf(sm2_vector_mean)\n",
    "    dff_sm2 = df_sm2.drop(columns = ['adm0_name', 'adm1_name','adm2_name', 'adm0_code', 'adm1_code','disp_area','geometry','status'])\n",
    "    dff_sm2_r = dff_sm2.rename(columns={'mode':'Mean_Prec'})\n",
    "    return dff_sm2_r\n",
    "\n",
    "def get_ecmwf():\n",
    "    data_ecmwf = ee.Image('users/wfphqgis/CLIM/ecmwf_reanalysis_ymonmeanR_ymonmean')\n",
    "    crs = data_ecmwf.projection().crs()\n",
    "    scale = 9000\n",
    "    tp = data_ecmwf.select('b' + s_bm)\n",
    "    ecmwf = tp.resample('bilinear').reproject(crs = crs, scale = scale)\n",
    "    wc_vector_mean = spatial_red(tp,ee.Reducer.mode(),9000)\n",
    "    df_wc = eeconvert.fcToGdf(wc_vector_mean)\n",
    "    dff_wc = df_wc.drop(columns = ['adm0_name', 'adm1_name','adm2_name', 'adm0_code', 'adm1_code','disp_area','geometry','status'])\n",
    "    dff_wc_r = dff_wc.rename(columns={'mode':'ECMWF_Mean_Prec'})\n",
    "    return dff_wc_r\n",
    "\n",
    "def get_amsr2():\n",
    "    #data = 'http://www.gdacs.org/flooddetection/DATA/AMSR2/AvgSignalTiffs/2019' 400K max\n",
    "    data = 'http://www.gdacs.org/flooddetection/DATA/AMSR2/MagTiffs/2019' #20K max\n",
    "    now = datetime.today()\n",
    "    d = str(now)\n",
    "    date = d[0:10]\n",
    "    date_frmt = date.replace('-', '')\n",
    "\n",
    "    page = requests.get(data)\n",
    "    webpage = html.fromstring(page.content)\n",
    "\n",
    "    amsr2 = webpage.xpath('//a/@href')\n",
    "    amsr2_last = amsr2[-1]\n",
    "    cur_year = d[0:4]\n",
    "    base = 'http://www.gdacs.org'\n",
    "    #prod = 'signal_4days_avg_4days_'\n",
    "    prod = 'mag_signal_'\n",
    "    hd5 = '_HD5'\n",
    "    ext = '.tif'\n",
    "    lnk =  base + amsr2_last + prod + date_frmt +hd5 +ext\n",
    "    dl = wget.download(lnk,process_amsr2)\n",
    "    return dl\n",
    "\n",
    "def get_s1_info(i):\n",
    "    s1 = api.query(final_df.envelope[i],\n",
    "                   date=sensing_date,\n",
    "                   platformname='Sentinel-1',\n",
    "                   producttype='GRD',\n",
    "                   area_relation='Intersects') #Intersects, IsWithin\n",
    "                   #orbitdirection='ASCENDING')\n",
    "    s1_json = api.to_geojson(s1)\n",
    "    s1_gdf = api.to_geodataframe(s1)\n",
    "    s1_gdf\n",
    "\n",
    "    s1_gdf = s1_gdf[['ingestiondate']]#,'beginposition','ingestiondate','platformname','orbitnumber', 'producttype', 'endposition']]\n",
    "    s1_gdf.sort_values('ingestiondate', ascending=True).head()\n",
    "    s1_gdf\n",
    "\n",
    "    dates = s1_gdf[\"ingestiondate\"].dt.date.values\n",
    "    map_date = dates + timedelta(days=11)\n",
    "    ld = len(dates)\n",
    "    \n",
    "    print_path = open(root + '/alert/' + 'sentinel_sched_' + date_cut + '.txt','a')\n",
    "    \n",
    "    p = print(\"Sentinel Acquisition Schedule: \" + \"{0}: {1} \\n{2} alert! \\nThere are {3} Sentinel-1 overpass \\nMappable on:\\n{4}\\n\"\n",
    "             .format(final_df['adm0_name'][i],final_df['adm2_name'][i],final_df['status'][i],ld,map_date),\n",
    "             file=print_path)\n",
    "    return p\n",
    "\n",
    "def job():\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = 'wfp.hq.dbtrack@gmail.com'\n",
    "    msg['Subject'] = \"Extreme Precipitation Alert\"\n",
    "    \n",
    "    body = \"\"\"<div style=\"color:rgb(28, 130, 196)\"><font size=\"+2\"><b><img id=\"myimage\" src=\"https://mw1.google.com/crisisresponse/icons/un-ocha/disaster_heavy_rain_100px_icon.png\"></img>\n",
    "    <br /> Extreme Precipitation Alert</b> </font></div>\n",
    "    <br/><b>Please See attached list and location of WFP country concerned.</b></b>\n",
    "    <br />\n",
    "    <br/> Number of <font color=\"YELLOW\">YELLOW</font> Alerts: %s\n",
    "    <br/> Number of <font color=\"ORANGE\">ORANGE</font> Alerts: %s\n",
    "    <br/> Number of <font color=\"RED\">RED</font> Alerts: %s\n",
    "    <br />\n",
    "    <br/><b>Note on color categories:</b></b>\n",
    "    <br />\n",
    "    <br/>Yellow: The 3-days accumulated rainfall is greater than the average 3 days maximum total precipitation (moderate nowcast)\n",
    "    <br/>Orange: The 3-days accumulated rainfall is greater than the long term mean rainfall for that month (high nowcast)\n",
    "    <br/>Red*: The 3-days accumulated rainfall + 3-days forecast is greater than the long term mean rainfall (high forecast)\n",
    "    <br/><i>*The red alert is valid until 3-5 days of forecast</i></b>\n",
    "    <br />\n",
    "    <br/><b>Emergency Mapping/Charter Activation Links:</b></b>\n",
    "    <br />\n",
    "    <br/><b>--------------------------------------------------------------------------------</b></b>\n",
    "    <br/>This is an automatically generated email, please do not reply.<br/>\n",
    "    <br/>Service provided by \n",
    "    <br/><i>UNITED NATIONS WORLD FOOD PROGRAMME\n",
    "    <br/>WFP Emergency Preparedness & Support Response Division\n",
    "    <br/>Contact: <a href=\"mailto:hq.gis@wfp.org\">HQ Geospatial Support Unit</a></i></p>\n",
    "    \"\"\" % (y,o,r) #add 'a' in the modulo\n",
    "\n",
    "    alert_fn = alert_csv_path + alert_data_name\n",
    "    mapview_fn = map_view_path + map_view_name\n",
    "    s1_fn = s1_txt_path + s1_name\n",
    "    \n",
    "    alerts = MIMEBase('application', \"octet-stream\")\n",
    "    mapview = MIMEBase('application', \"octet-stream\")\n",
    "    s1view = MIMEBase('application', \"octet-stream\")\n",
    "    \n",
    "    alerts.set_payload(open(alert_fn, \"rb\").read())\n",
    "    mapview.set_payload(open(mapview_fn, \"rb\").read())\n",
    "    s1view.set_payload(open(s1_fn, \"rb\").read())\n",
    "    \n",
    "    encoders.encode_base64(alerts)\n",
    "    encoders.encode_base64(mapview)\n",
    "    encoders.encode_base64(s1view)\n",
    "    \n",
    "    alerts.add_header('Content-Disposition', 'attachment; filename=%s'%alert_data_name)\n",
    "    mapview.add_header('Content-Disposition', 'attachment; filename=%s'%map_view_name)\n",
    "    s1view.add_header('Content-Disposition', 'attachment; filename=%s'%s1_name)\n",
    "    \n",
    "    msg.attach(alerts)\n",
    "    msg.attach(mapview)\n",
    "    msg.attach(s1view)\n",
    "    \n",
    "    msg.attach(MIMEText(body, 'html'))\n",
    "    \n",
    "#     s = smtplib.SMTP('smtp.gmail.com', 587) #Anywhere\n",
    "#     s.starttls() #Anywhere\n",
    "    \n",
    "    s = smtplib.SMTP_SSL('smtp.gmail.com', 465) # WFP domain\n",
    "    \n",
    "    s.ehlo()\n",
    "    s.login(smtp_un, smtp_pw)\n",
    "    sender = 'wfp.hq.dbtrack@gmail.com'\n",
    "    \n",
    "    recipients = ['michaelandrew.manalili@gmail.com']#,'michael.manalili@wfp.org','abdel-lathif.younous@wfp.org','rohini.swaminathan@wfp.org']\n",
    "    #recipients = ['rohini.swaminathan@wfp.org','sirio.modugno@wfp.org','stefano.cairo@wfp.org','michael.manalili@wfp.org','abdel-lathif.younous@wfp.org','michaelandrew.manalili@gmail.com']\n",
    "            \n",
    "    if gbl_alert.empty == True:\n",
    "        pass\n",
    "        print('DataFrame is Empty, skipping email broadcast...')\n",
    "        \n",
    "    else:\n",
    "        s.sendmail(sender, recipients, str(msg))\n",
    "        s.quit()\n",
    "        print('ALERT sent to subscribers!')\n",
    "\n",
    "#Flood: https://mw1.google.com/crisisresponse/icons/un-ocha/disaster_flood_100px_icon.png\n",
    "#http://www.gdacs.org/flooddetection/DATA/AMSR2/\n",
    "#http://www.gdacs.org/flooddetection/DATA/AMSR2/SignalTiffs/2019/\n",
    "#http://www.gdacs.org/flooddetection/DATA/AMSR2/MagTiffs/2019/\n",
    "\n",
    "# if mode == 'prod':\n",
    "#             mailServer = smtplib.SMTP(\"smtp.gmail.com\", 587)\n",
    "#         else:### in virtual machines security rules don't allow anymore using  (old version) TLS connection, but only SSL. In production, SSL version is good enough\n",
    "#             mailServer = smtplib.SMTP_SSL(\"smtp.gmail.com\", 465)\n",
    "\n",
    "### Add the below to the message to include GIS focal points\n",
    "# <br /><b>GIS Focal point for coordinating the event:</b>\n",
    "# <br />\n",
    "# </b>%s\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "iso_global = pd.read_csv(root + '/wfp' + '/wfp_country.csv', index_col=0)\n",
    "iso_global_list = iso_global['ISO3'].tolist()\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "# Add population to:\n",
    "#Forecast + XDAYS + Population (Abdel to advise on this number)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "GBL_adm0 = ee.FeatureCollection(\"users/wfphqgis/BND/wfp_bnd_inform2019\") #iso3\n",
    "GBL_adm2 = ee.FeatureCollection(\"users/wfphqgis/BND/admin_2_gaul_2015\") #admX_name\n",
    "#GBL_dfo = ee.FeatureCollection(\"users/wfphqgis/FLOOD/DFO_HistoricalFloodEvents\")\n",
    "GBL_dfo = ee.FeatureCollection(\"users/wfphqgis/FLOOD/DFO_HistoricalFloodEvents_M\")\n",
    "\n",
    "# wfp_adm0 = GBL_adm0.filter(ee.Filter.inList\n",
    "#                            ('iso3',['DRC','NIC','HND','GTM','CAF','IRQ','LBY','BGD','MYM','COL','CIV',\n",
    "#                                     'BFA','SSD','YEM','SYR','CAM','NIG','DRC','MOZ','PHL','LBN','LKA',\n",
    "#                                     'ZWE','BEN','TGO','GHA','TUR','IND','MLI','PAN','SOM','TCD','NGA',\n",
    "#                                     'IDN','GIN','PAK','TZA','IRN','TJK','AFG','IRQ','SLE','VEN','VNM',\n",
    "#                                     'CMR','GMB','GNB','LBR','MRT','NER','SDN','MRT','MMR','LAO']))\n",
    "\n",
    "wfp_adm0 = GBL_adm0.filter(ee.Filter.inList\n",
    "                           ('iso3',['PHL']))\n",
    "\n",
    "# L3\n",
    "# ['SSD','YEM','SYR','CAM','NIG','DRC','MOZ']\n",
    "# L2\n",
    "# ['MLI','CAF','IRQ','LBY','BGD','MYM','COL', 'BFA']\n",
    "# M\n",
    "# ['DRC','NIC','HND','GTM']\n",
    "# Other\n",
    "# ['DRC','NIC','HND','GTM','MLI','CAF','IRQ','LBY','BGD','MYM','COL',\n",
    "# 'BFA','SSD','YEM','SYR','CAM','NIG','DRC','MOZ','PHL','LBN','LKA',\n",
    "# 'ZWE','BEN','TGO','GHA','TUR','IND','MLI','PAN','SOM','TCD','NGA',\n",
    "# 'IDN','GIN','PAK','TZA','IRN','TJK','AFG','IRQ','SLE']\n",
    "\n",
    "# Should work globally..\n",
    "# wfp_adm0 = GBL_adm0.filter(ee.Filter.inList\n",
    "#                            ('iso3',iso_global_list))\n",
    "\n",
    "# Togo, Benin, Ghana, Mali, Codivor, SSD, CAR, PANAMA\n",
    "\n",
    "dfo = GBL_dfo.filterBounds(wfp_adm0)\n",
    "wfp_aoi = GBL_adm2.filterBounds(dfo)\n",
    "dfodata = eeconvert.fcToGdf(GBL_dfo)\n",
    "\n",
    "\n",
    "# ## GEE data calls\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "GSMaP = ee.ImageCollection('JAXA/GPM_L3/GSMaP/v6/operational')\n",
    "precipitation = GSMaP.select('hourlyPrecipRate')\n",
    "\n",
    "worldpop = ee.ImageCollection('WorldPop/POP').filter(ee.Filter.equals('year', 2015)).filter(ee.Filter.equals('UNadj', 'yes'))\n",
    "wpop2015 = worldpop.select('population').reduce(ee.Reducer.max()).clip(wfp_adm0)\n",
    "\n",
    "ciesn2020 = ee.ImageCollection('CIESIN/GPWv4/unwpp-adjusted-population-count')\n",
    "ciesn2020_pop = ciesn2020.select('population-count').reduce(ee.Reducer.max()).clip(wfp_adm0)\n",
    "\n",
    "jrc20yrp = ee.Image('users/wfphqgis/HAZ/floodMapGL_rp20y')\n",
    "jrc_1m_remap = jrc20yrp.lte(14)\n",
    "jrc_1m_depth = jrc_1m_remap.remap([1,2,3,4,5,6,7,8,9,10,11,12,13,14],\n",
    "                                  [1,1,1,1,1,1,1,1,1,1,1,1,1,1])\n",
    "\n",
    "hectares = ee.Image.pixelArea().divide(10000)\n",
    "\n",
    "hectares_jrc = jrc20yrp.multiply(hectares)\n",
    "\n",
    "#pop_haz_area = wpop2015.multiply(jrc_1m_depth)\n",
    "pop_haz_area = ciesn2020_pop.multiply(jrc_1m_depth)\n",
    "\n",
    "#Change\n",
    "pxr = ee.Image('users/wfphqgis/CLIM/Global_IDF')\n",
    "pxr_3days = pxr.select('b12') #band 12 for 72hours (3Days) band 14 is 120hours (5days) and \n",
    "pxr_3days_vector = spatial_red(pxr_3days,ee.Reducer.mode(),30000)\n",
    "\n",
    "#Uncomment for GFS data (update datetime is not yet fixed)\n",
    "#gfs_fc = ee.ImageCollection('NOAA/GFS0P25').select(\"total_precipitation_surface\").filter(ee.Filter.eq('creation_time',ee.Date(0).update(2019,7,21,6,0,0)\n",
    "#                                                                                                      .millis())).filter(ee.Filter.eq('forecast_hours',72)).sum()\n",
    "#gfs_fc72h_res = gfs_fc.resample('bilinear').reproject(crs=gfs_fc.projection(), scale=10000).clip(wfp_adm0)\n",
    "\n",
    "#vam = ee.Image('users/wfphqgis/CLIM/wld_cli_rainfall_threshold_q96_25yr_chirps_wfp')\n",
    "#vam_vector = spatial_red(pxr_3days,ee.Reducer.mode(),5000)\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "#GEE Datetime for GPM accumulated Precipitation\n",
    "day0 = datetime.today()\n",
    "hrs24 = day0 - timedelta(days=1)\n",
    "hrs48 = day0 - timedelta(days=2)\n",
    "hrs72 = day0 - timedelta(days=3)\n",
    "day7 = day0 - timedelta(days=7)\n",
    "\n",
    "#Temporal Reducer\n",
    "gpm_precip_24h = precipitation.filter(ee.Filter.date(hrs24, day0))\n",
    "gpm_precip_48h = precipitation.filter(ee.Filter.date(hrs48, day0))\n",
    "gpm_precip_72h = precipitation.filter(ee.Filter.date(hrs72, day0))\n",
    "gpm_precip_day7 = precipitation.filter(ee.Filter.date(day7, day0))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#Changed to WorldClimV2\n",
    "band_month = datetime.today().strftime(\"%m\")\n",
    "#band_month = strptime(run_date,'%b').tm_mon\n",
    "s_bm = str(band_month)\n",
    "dff_wc_rename = get_wc()\n",
    "#dff_ecmwf_rename = get_ecmwf()\n",
    "dff_sm2_rename = get_sm2map()\n",
    "print('Climate Data loaded..')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#Spatial Reducer\n",
    "gpm72h = gpm_precip_72h.sum().rename('72h').clip(wfp_aoi)\n",
    "gsmap_vector_mean = spatial_red(gpm72h,ee.Reducer.mode(),500)\n",
    "wpop_vector = spatial_red(wpop2015,ee.Reducer.sum(),95)\n",
    "jrc_vector = spatial_red(jrc_1m_depth,ee.Reducer.sum(),1000) #1km resolution all models\n",
    "exp_vector = spatial_red(pop_haz_area,ee.Reducer.sum(),1000) #95 for WorldPop and 1000 for CIESN\n",
    "\n",
    "#Change\n",
    "ciesn_vector = spatial_red(ciesn2020_pop,ee.Reducer.sum(),1000)\n",
    "#gfs_fc72h_vector = spatial_red(gfs_fc72h_res,ee.Reducer.mode(),10000)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "print('Computing Global parameters. Please wait...')\n",
    "df_precip_m = eeconvert.fcToGdf(gsmap_vector_mean)\n",
    "df_pop = eeconvert.fcToGdf(wpop_vector)\n",
    "df_jrc = eeconvert.fcToGdf(jrc_vector)\n",
    "df_exp = eeconvert.fcToGdf(exp_vector)\n",
    "\n",
    "#Change\n",
    "df_pxr = eeconvert.fcToGdf(pxr_3days_vector)\n",
    "df_pop1km = eeconvert.fcToGdf(ciesn_vector)\n",
    "#df_gfs = eeconvert.fcToGdf(gfs_fc72h_vector)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#ECMWF datetime format for FTP (Temporary Solution while migrating to Windows)\n",
    "forecast = datetime.today()\n",
    "fc_fmt = forecast.strftime(\"%m%d\")\n",
    "days3 = forecast + timedelta(days=3)\n",
    "days5 = forecast + timedelta(days=5)\n",
    "days7 = forecast + timedelta(days=7)\n",
    "\n",
    "days3F = days3.strftime(\"%m%d\")\n",
    "days5F = days5.strftime(\"%m%d\")\n",
    "days7F = days7.strftime(\"%m%d\")\n",
    "\n",
    "ecmwf_3D_fc = fc_fmt + days3F + '00' + 'TP' + '.tiff'\n",
    "ecmwf_5D_fc = fc_fmt + days5F + '00' + 'TP' + '.tiff'\n",
    "ecmwf_7D_fc = fc_fmt + days7F + '00' + 'TP' + '.tiff'\n",
    "\n",
    "config.read(os.path.join(root, \"config.txt\"))\n",
    "ftp_user = config.get('gisftp', 'ftp_un')\n",
    "ftp_pw = config.get('gisftp', 'ftp_pw')\n",
    "ftp_gis =  config.get('gisftp', 'ftp_url')\n",
    "ftp = FTP(ftp_gis)\n",
    "ftp.login(user=ftp_user, passwd = ftp_pw)\n",
    "save2local = ftp.cwd(\"/ECMWF_processed\")\n",
    "\n",
    "def grabFile(fn):\n",
    "    filename = fn\n",
    "    localfile = open(process_ecmwf + filename, 'wb')\n",
    "    return ftp.retrbinary('RETR ' + filename, localfile.write, 1024)\n",
    "\n",
    "grabFile(ecmwf_3D_fc)\n",
    "grabFile(ecmwf_5D_fc)\n",
    "grabFile(ecmwf_7D_fc)\n",
    "\n",
    "ftp.quit()\n",
    "\n",
    "\n",
    "# ### Alert data generation\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "dff_precip = df_precip_m.drop(columns = ['adm0_code', 'adm1_code','disp_area','status'])\n",
    "dff_p_rename = dff_precip.rename(columns={'mode':'3D_acc_precip'})\n",
    "dff_pop = df_pop.drop(columns = ['adm0_name', 'adm1_name','adm2_name', 'adm0_code', 'adm1_code','disp_area','geometry','status'])\n",
    "dff_pop_rename = dff_pop.rename(columns={'sum':'Pop_adm2_WPOP2015'})\n",
    "\n",
    "dff_jrc = df_jrc.drop(columns = ['adm0_name', 'adm1_name','adm2_name', 'adm0_code', 'adm1_code','disp_area','geometry','status'])\n",
    "dff_jrc_rename = dff_jrc.rename(columns={'sum':'JRC_Flood_Haz'})\n",
    "\n",
    "dff_exp = df_exp.drop(columns = ['adm0_name', 'adm1_name','adm2_name', 'adm0_code', 'adm1_code','disp_area','geometry','status'])\n",
    "dff_exp_rename = dff_exp.rename(columns={'sum':'Exp_Pop_1m'})\n",
    "\n",
    "#Change PXR RFIDF\n",
    "dff_pxr = df_pxr.drop(columns = ['adm0_name', 'adm1_name','adm2_name', 'adm0_code', 'adm1_code','disp_area','geometry','status'])\n",
    "dff_pxr_rename = dff_pxr.rename(columns={'mode':'MaxTP_3D'})\n",
    "\n",
    "dff_pop1km = df_pop1km.drop(columns = ['adm0_name', 'adm1_name','adm2_name', 'adm0_code', 'adm1_code','disp_area','geometry','status'])\n",
    "dff_pop1km_rename = dff_pop1km.rename(columns={'sum':'Pop_adm2_CSN2019'})\n",
    "\n",
    "#dff_gfs = df_gfs.drop(columns = ['adm0_name', 'adm1_name','adm2_name', 'adm0_code', 'adm1_code','disp_area','geometry','status'])\n",
    "#df_gfs_rename = dff_gfs.rename(columns={'mode':'GFS_3D'})\n",
    "\n",
    "#Merge all DF\n",
    "pre_merged = dff_p_rename.merge(dff_sm2_rename, on='adm2_code').merge(dff_pop_rename,on='adm2_code').merge(dff_jrc_rename,on='adm2_code').merge(dff_exp_rename,on='adm2_code').merge(dff_pxr_rename,on='adm2_code').merge(dff_pop1km_rename,on='adm2_code')#.merge(df_gfs_rename,on='adm2_code')\n",
    "pre_merged['MaxTP_3D'] = pre_merged['MaxTP_3D'] * 1000\n",
    "pre_merged = pre_merged.round(3)\n",
    "\n",
    "\n",
    "\n",
    "### For local testing not connected to WFP domain (should have data locally)\n",
    "#ecmwf_F_3D = '/Users/michael/GEO/adam-floods-alert/processing/process_ecmwf/0830090200TP.tiff'\n",
    "#ecmwf_F_5D = '/Users/michael/GEO/adam-floods-alert//processing/process_ecmwf/0830090400TP.tiff'\n",
    "\n",
    "### Uncomment for NRT\n",
    "ecmwf_F_3D = process_ecmwf + ecmwf_3D_fc\n",
    "ecmwf_F_5D = process_ecmwf + ecmwf_5D_fc\n",
    "ecmwf_F_3D_stats = zonal_stats(pre_merged, ecmwf_F_3D, prefix='ecmwf3D_',geojson_out=True)\n",
    "ecmwf_F_5D_stats = zonal_stats(pre_merged, ecmwf_F_5D, prefix='ecmwf5D_',geojson_out=True)\n",
    "ecmwf_F_3D_gdf = GeoDataFrame.from_features(ecmwf_F_3D_stats)\n",
    "ecmwf_F_5D_gdf = GeoDataFrame.from_features(ecmwf_F_5D_stats)\n",
    "\n",
    "ecmwf_F_3D_gdf['ecmwf3D_mean'] = ecmwf_F_3D_gdf['ecmwf3D_mean'] * 1000 # converts TP values from meters to mm\n",
    "ecmwf_F_5D_gdf['ecmwf5D_mean'] = ecmwf_F_5D_gdf['ecmwf5D_mean'] * 1000 # converts TP values from meters to mm\n",
    "\n",
    "\n",
    "dfNew = ecmwf_F_5D_gdf.merge(ecmwf_F_3D_gdf, left_index=True, right_index=True,\n",
    "                 how='outer', suffixes=('', '_y'))\n",
    "dfNew.drop(list(dfNew.filter(regex='_y$')), axis=1, inplace=True)\n",
    "to_drop = ['ecmwf5D_count','ecmwf5D_max','ecmwf5D_min','ecmwf3D_count','ecmwf3D_max','ecmwf3D_min']\n",
    "gbl_alert = dfNew.drop(columns=to_drop).round(2)\n",
    "gbl_alert['uuid'] = [uuid.uuid4() for _ in range(len(gbl_alert.index))]\n",
    "print('Global Alerts created...')\n",
    "#gbl_alert = gbl_alert.loc[(gbl_alert['3D_acc_precip'] >= 50)]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "### Before 19 June 2019\n",
    "#map_me = alert.loc[(alert['status'] == 'red') | (alert['P100'] == True) | (alert['ecmwf3D_mean'] > 50)]\n",
    "\n",
    "### Deployed version 19 June 2019\n",
    "#map_me = alert.loc[((alert['status'] == 'green') & (alert['P60'] == True)) | ((alert['status'] == 'red') & (alert['ecmwf3D_mean'] > alert['Mean_Prec']*0.6))] \n",
    "\n",
    "#map_me = alert.loc[(alert['3D_acc_precip'] > alert['MaxInt_3D_x']) | alert['P80'] == True]\n",
    "#map_me.head()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# amsr2 = get_amsr2()\n",
    "# amsr2_stats = zonal_stats(alert, amsr2, prefix='amsr2',geojson_out=True)\n",
    "# zonal_ppt_gdf = GeoDataFrame.from_features(amsr2_stats)\n",
    "# zonal_ppt_gdf = zonal_ppt_gdf.loc[(zonal_ppt_gdf['amsr2max'] > 10000)]\n",
    "# zonal_ppt_gdf.head()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def func1(x):\n",
    "    if x >= 50000:\n",
    "        return \"red\"\n",
    "    elif x <= 10000:\n",
    "        return \"green\"\n",
    "    else:\n",
    "        return \"orange\"\n",
    "    \n",
    "def func2(x):\n",
    "    if x >= 500000:\n",
    "        return \"red\"\n",
    "    elif x <= 10000:\n",
    "        return \"green\"\n",
    "    else:\n",
    "        return \"orange\"\n",
    "\n",
    "# def func3(df):\n",
    "#     if df['nowcast_mod'] == True & df['pop_status'] == 'red':\n",
    "#         return \"green\"\n",
    "#     elif df['nowcast_high'] == True & df['pop_status'] == 'red':\n",
    "#         return \"orange\"\n",
    "#     elif df['forecast_high'] == True & df['pop_status'] == 'red':\n",
    "#         return \"red\"\n",
    "#     else:\n",
    "#         pass\n",
    "# gbl_alert['status'] = gbl_alert.apply(func3)\n",
    "\n",
    "#ABDELS COMMENTS\n",
    "#map_me = alert.loc[((alert['3D_acc_precip'] + alert['ecmwf3D_mean']) > alert['Mean_Prec'])] \n",
    "\n",
    "#Conditions goes here. Needs more work here\n",
    "#gbl_alert = gbl_alert.loc[gbl_alert['3D_acc_precip'] >=50]\n",
    "gbl_alert['pop_status'] = gbl_alert['Pop_adm2_CSN2019'].apply(func2)\n",
    "gbl_alert['exp_status'] = gbl_alert['Exp_Pop_1m'].apply(func1)\n",
    "\n",
    "#gbl_alert['P60'] = (gbl_alert['3D_acc_precip'] > gbl_alert['Mean_Prec']*0.60)\n",
    "#gbl_alert['P80'] = (gbl_alert['3D_acc_precip'] > gbl_alert['Mean_Prec']*0.80)\n",
    "#gbl_alert['P100'] = (gbl_alert['3D_acc_precip'] > gbl_alert['Mean_Prec'])\n",
    "#alert['ecmwf'] = alert['ECMWF_Mean_Prec'].apply(lambda x: x*1000) #check with abdel TP values\n",
    "\n",
    "#alert = alert[['adm0_name','adm1_name','adm2_name','adm2_code','status','MaxTP_3D','Mean_Prec','3D_acc_precip','ecmwf3D_mean','ecmwf5D_mean','Est_pop_adm2','JRC_Flood_Haz','Exp_Pop_1m','P60','P80','P100']]\n",
    "\n",
    "#Condition 1\n",
    "#gbl_alert['nowcast_low'] = (gbl_alert['3D_acc_precip'] > gbl_alert['MaxTP_3D']).astype(int)\n",
    "#Condition 2\n",
    "#gbl_alert['nowcast_mod'] = ((gbl_alert['3D_acc_precip'] + gbl_alert['ecmwf3D_mean']) > gbl_alert['MaxTP_3D']).astype(int)\n",
    "gbl_alert['nowcast_mod'] = (gbl_alert['3D_acc_precip'] > gbl_alert['MaxTP_3D']).astype(int)\n",
    "#Condition 3\n",
    "gbl_alert['nowcast_high'] = (gbl_alert['3D_acc_precip'] > gbl_alert['Mean_Prec']).astype(int)\n",
    "#Condition 3\n",
    "gbl_alert['forecast_high'] = ((gbl_alert['3D_acc_precip'] + gbl_alert['ecmwf3D_mean'] > (gbl_alert['Mean_Prec'] + gbl_alert['Mean_Prec']*0.60)) | (gbl_alert['3D_acc_precip'] + gbl_alert['ecmwf5D_mean'] > (gbl_alert['Mean_Prec'] + gbl_alert['Mean_Prec']*0.60))).astype(int)\n",
    "#gbl_alert = gbl_alert.loc[(gbl_alert['nowcast_mod'] == True & gbl_alert['pop_status'] == 'red') | (gbl_alert['nowcast_high'] == True) | (gbl_alert['forecast_high'] == True)]\n",
    "\n",
    "#People Living only\n",
    "# gbl_alert = gbl_alert.loc[((gbl_alert['nowcast_mod'] == True) & (gbl_alert['pop_status'] == 'red') | \n",
    "#                            (gbl_alert['nowcast_high'] == True) & (gbl_alert['pop_status'] == 'red') | \n",
    "#                            (gbl_alert['forecast_high'] == True) & (gbl_alert['pop_status'] == 'red'))]\n",
    "\n",
    "##Exposure\n",
    "gbl_alert.loc[(gbl_alert['3D_acc_precip'] >= 50), 'status'] = 'white'\n",
    "gbl_alert.loc[(gbl_alert['nowcast_mod'] == True) & (gbl_alert['exp_status'] == 'red'),'status']  = 'yellow'\n",
    "gbl_alert.loc[(gbl_alert['nowcast_high'] == True) & (gbl_alert['exp_status'] == 'red'), 'status'] = 'orange'\n",
    "gbl_alert.loc[(gbl_alert['forecast_high'] == True) & (gbl_alert['exp_status'] == 'red'), 'status'] = 'red'\n",
    "final_df = gbl_alert.loc[(gbl_alert['status'] == 'yellow') | (gbl_alert['status'] == 'orange')| (gbl_alert['status'] == 'red')| (gbl_alert['status'] == 'white')]\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "summary = final_df.groupby('status')['status'].value_counts()\n",
    "sss = summary.values.tolist()\n",
    "\n",
    "try:\n",
    "    w = ''\n",
    "    y = ''\n",
    "    o = ''\n",
    "    r = ''\n",
    "    w = summary[0]\n",
    "    y = summary[1]\n",
    "    o = summary[2]\n",
    "    r = summary[3]\n",
    "except Exception:\n",
    "    pass \n",
    "\n",
    "### GIS Officers in the area    \n",
    "# recip = pd.read_excel(root + '/wfp' + '/wfp_gis.xlsx', index_col=0, sheet_name='GIS_officer')\n",
    "# input_countries = map_me['adm0_name'].to_list()\n",
    "# countries = {}\n",
    "# for country in pycountry.countries:\n",
    "#     countries[country.name] = country.alpha_3\n",
    "# c = [countries.get(c, 'Unknown code') for c in input_countries]\n",
    "# codes = list(set(c))\n",
    "# codes_df = pd.DataFrame(codes,columns=['ISO3'])\n",
    "\n",
    "# hoomans = []\n",
    "\n",
    "# for i in codes:\n",
    "#     gis_officers = recip.loc[recip['ISO3']==i]    \n",
    "#     gis_officers_ls = gis_officers.values.tolist()\n",
    "#     #gis_officer_dict = dict(gis_officers_ls)\n",
    "#     hoomans.append(gis_officers_ls)   \n",
    "# list2 = [x for x in hoomans if x != []]\n",
    "# l = [i.strip('[]') if type(i) == str else str(i) for i in list2]\n",
    "# a = \"\\n\".join(l)\n",
    "# a\n",
    "\n",
    "\n",
    "geo = pre_merged.merge(final_df, on='adm2_code')\n",
    "geo.drop(list(geo.filter(regex='_y$')), axis=1, inplace=True)\n",
    "geo = geo.rename(columns={'geometry_x':'geometry'})\n",
    "#geo.tail()\n",
    "\n",
    "\n",
    "# ### Save WHITE alerts to DB\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "config.read(os.path.join(root, \"config.txt\"))\n",
    "dbserver_out = config.get('db_out','dbserver')\n",
    "dbname_out = config.get('db_out','dbname')\n",
    "dbuser_out = config.get('db_out','dbuser')\n",
    "dbpword_out = config.get('db_out','dbpword')\n",
    "dbport_out = config.get('db_out','dbport')\n",
    "engine = create_engine('postgresql+psycopg2://' + dbuser_out + ':' + dbpword_out + '@' + dbserver_out + ':' + dbport_out + '/' + dbname_out)\n",
    "\n",
    "try:\n",
    "    df = pd.DataFrame(final_df)\n",
    "    df.head(0).to_sql(whitelist_data, engine, if_exists='replace',index=False) #truncates the table\n",
    "    conn = engine.raw_connection()\n",
    "    cur = conn.cursor()\n",
    "    output = io.StringIO()\n",
    "    df.to_csv(output, sep='\\t', header=False, index=False)\n",
    "    output.seek(0)\n",
    "    contents = output.getvalue()\n",
    "    cur.copy_from(output, whitelist_data, null=\"\") # null values become ''\n",
    "    conn.commit()\n",
    "\n",
    "    sqlstr = \"ALTER TABLE {table_name} ALTER COLUMN geometry TYPE geometry;\".format(** {\n",
    "                'table_name': whitelist_data})\n",
    "\n",
    "    cur.execute(sqlstr)\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    print('White Alerts Sent to DB')\n",
    "except Exception:\n",
    "    pass \n",
    "\n",
    "\n",
    "# ### Save RED alerts to flood.db\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "sensing_date=('NOW-11DAYS',date_cut) #XDAYS XMONTH XWEEK\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "config.read(os.path.join(root, \"config.txt\"))\n",
    "dbserver_al = config.get('db_alert','dbserver_a')\n",
    "dbname_al = config.get('db_alert','dbname_a')\n",
    "dbuser_al = config.get('db_alert','dbuser_a')\n",
    "dbpword_al = config.get('db_alert','dbpword_a')\n",
    "dbport_al = config.get('db_alert','dbport_a')\n",
    "alert_engine = create_engine('postgresql+psycopg2://' + dbuser_al + ':' + dbpword_al + '@' + dbserver_al + ':' + dbport_al + '/' + dbname_al)\n",
    "\n",
    "try:\n",
    "    if final_df.empty == False: \n",
    "        df_alert = pd.DataFrame(final_df)\n",
    "        df_alert.head(0).to_sql(alert_data, alert_engine, if_exists='replace',index=False) #truncates the table\n",
    "        conn2 = alert_engine.raw_connection()\n",
    "        cur2 = conn2.cursor()\n",
    "        output2 = io.StringIO()\n",
    "        df_alert.to_csv(output2, sep='\\t', header=False, index=False)\n",
    "        output2.seek(0)\n",
    "        contents2 = output2.getvalue()\n",
    "        cur2.copy_from(output2, alert_data, null=\"\") # null values become ''\n",
    "        conn2.commit()\n",
    "\n",
    "        sqlstr2 = \"ALTER TABLE {table_name} ALTER COLUMN geometry TYPE geometry;\".format(** {\n",
    "                    'table_name': alert_data})\n",
    "\n",
    "        cur2.execute(sqlstr2)\n",
    "        conn2.commit()\n",
    "        cur2.close()\n",
    "        print('Orange Alerts Sent to DB')\n",
    "        \n",
    "    else:\n",
    "        print('No Alerts for today. Check White alerts if necessary!')\n",
    "except Exception:\n",
    "    pass \n",
    "\n",
    "try:\n",
    "    if final_df.empty == True:\n",
    "        pass\n",
    "    else:\n",
    "        bbox = final_df.envelope\n",
    "        bbox_gdf = gpd.GeoDataFrame(gpd.GeoSeries(bbox), columns=['geometry'])\n",
    "        c = pd.concat([bbox_gdf,final_df],axis=1)\n",
    "        x = bbox.to_json()\n",
    "        m = folium.Map(tiles='cartodbpositron') #cartodbpositron #stamentoner #cartodbdark_matter\n",
    "        folium.GeoJson(x).add_to(m)\n",
    "        m.fit_bounds(m.get_bounds())\n",
    "        map_view_name = 'Map_view_' + date_cut + '.html'\n",
    "        view = m.save(map_view_path + map_view_name)\n",
    "        alert_geo = pd.DataFrame(final_df)\n",
    "        alert_content = alert_geo.drop(columns = ['geometry'])\n",
    "        att_alert = alert_content.to_csv(alert_csv_path + alert_data_name)\n",
    "        \n",
    "        recip = pd.read_excel(root + '/wfp' + '/wfp_gis.xlsx', index_col=0, sheet_name='GIS_officer')\n",
    "        input_countries = final_df['adm0_name'].to_list()\n",
    "        countries = {}\n",
    "        for country in pycountry.countries:\n",
    "            countries[country.name] = country.alpha_3\n",
    "        c = [countries.get(c, 'Unknown code') for c in input_countries]\n",
    "        codes = list(set(c))\n",
    "        codes_df = pd.DataFrame(codes,columns=['ISO3'])\n",
    "\n",
    "        hoomans = []\n",
    "        summary = final_df.groupby('status')['status'].value_counts()\n",
    "        sss = summary.values.tolist()\n",
    "        \n",
    "        #grouped = final_df.groupby('status')\n",
    "        #grouped = final_df.groupby('nowcast_high')#['nowcast_high'].value_counts()\n",
    "        #s_nc = grouped.values.tolist()\n",
    "        #summary_fc = gbl_alert.groupby('forecast_high')#['forecast_high'].value_counts()\n",
    "        #s_fc = summary_fc.values.tolist()\n",
    "        #for name,group in grouped:\n",
    "\n",
    "        for index, row in final_df.iterrows():\n",
    "            if row.status == 'orange':\n",
    "                get_s1_info(index)\n",
    "            if row.status == 'red':\n",
    "                get_s1_info(index)\n",
    "        \n",
    "        for i in codes:\n",
    "            gis_officers = recip.loc[recip['ISO3']==i]    \n",
    "            gis_officers_ls = gis_officers.values.tolist()\n",
    "            #gis_officer_dict = dict(gis_officers_ls)\n",
    "            hoomans.append(gis_officers_ls)   \n",
    "        list2 = [x for x in hoomans if x != []]\n",
    "        l = [i.strip('[]') if type(i) == str else str(i) for i in list2]\n",
    "        a = \"\\n\".join(l)\n",
    "        job()\n",
    "                \n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"The script took {0} seconds to complete...\".format(time.time() - execTime))\n",
    "\n",
    "\n",
    "# ### End of script..\n",
    "\n",
    "\n",
    "# ## Working scratch do not delete!\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "### SCRATCH\n",
    "# conditions = [(df['column_1'] > 5),\n",
    "#               (df['column_1'] <= 5) & (df['column_1'] > 0),\n",
    "#               (df['column_1'] == 0)]\n",
    "\n",
    "# choices = ['high','low','null']\n",
    "\n",
    "# df['column_2'] = np.select(conditions, choices, default='null')\n",
    "\n",
    "# summary_nc = gbl_alert.groupby('nowcast_high')['nowcast_high'].value_counts()\n",
    "# s_nc = summary_nc.values.tolist()\n",
    "# summary_fc = gbl_alert.groupby('forecast_high')['forecast_high'].value_counts()\n",
    "# s_fc = summary_fc.values.tolist()\n",
    "# try:\n",
    "#     nc = ''\n",
    "#     fc = ''\n",
    "#     nc = s_nc[1]\n",
    "#     fc = s_fc[1]\n",
    "# except Exception:\n",
    "#     pass\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "### This works OK. \n",
    "# for name,group in grouped_nc:\n",
    "#     for i in codes:\n",
    "#         gis_officers = recip.loc[recip['ISO3']==i]    \n",
    "#         gis_officers_ls = gis_officers.values.tolist()\n",
    "#         #gis_officer_dict = dict(gis_officers_ls)\n",
    "#         hoomans.append(gis_officers_ls)   \n",
    "#     list2 = [x for x in hoomans if x != []]\n",
    "#     l = [i.strip('[]') if type(i) == str else str(i) for i in list2]\n",
    "#     a = \"\\n\".join(l)\n",
    "#     job()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# i = 0\n",
    "# while i == 0:\n",
    "#     if name == 'red':\n",
    "#         i = 0\n",
    "#         print('i am red')\n",
    "#     elif name == 'orange':\n",
    "#         i = 0\n",
    "#         print('i am orange')\n",
    "#     else:\n",
    "#         i += 1\n",
    "#         print('i am green')\n",
    "                \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# for name,group in grouped:\n",
    "#     if name == 'green':\n",
    "#         for i in codes:\n",
    "#             gis_officers = recip.loc[recip['ISO3']==i]    \n",
    "#             gis_officers_ls = gis_officers.values.tolist()\n",
    "#             #gis_officer_dict = dict(gis_officers_ls)\n",
    "#             hoomans.append(gis_officers_ls)   \n",
    "#         list2 = [x for x in hoomans if x != []]\n",
    "#         l = [i.strip('[]') if type(i) == str else str(i) for i in list2]\n",
    "#         a = \"\\n\".join(l)\n",
    "#         job()\n",
    "#     else:\n",
    "#         pass\n",
    "\n",
    "\n",
    "# ### Email works via wfp domain\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# alert_geo = pd.DataFrame(map_me)\n",
    "# alert_content = alert_geo.drop(columns = ['geometry'])\n",
    "# att_alert = alert_content.to_csv(alert_csv_path + alert_data_name)\n",
    "\n",
    "# #map_view_name = 'Map_view_' + date_cut + '.html'\n",
    "\n",
    "# job()\n",
    "\n",
    "# # try:\n",
    "# #     job()\n",
    "# # except Exception:\n",
    "# #     pass \n",
    "\n",
    "# execTime = time.time()\n",
    "# print(\"The script took {0} seconds to complete...\".format(time.time() - execTime))\n",
    "\n",
    "\n",
    "# ### Request HTA Map tiles\n",
    "\n",
    "# ### Get HTA Tiles of the alerted Area (wld_grid_100ka1_wfp) @ 43\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# config.read(os.path.join(root, \"config.txt\"))\n",
    "# dbserver_hta = config.get('hta','dbserver_hta')\n",
    "# dbname_hta = config.get('hta','dbname_hta')\n",
    "# dbuser_hta = config.get('hta','dbuser_hta')\n",
    "# dbpword_hta = config.get('hta','dbpword_hta')\n",
    "# dbport_hta = config.get('hta','dbport_hta')\n",
    "# hta_engine = create_engine('postgresql+psycopg2://' + dbuser_hta + ':' + dbpword_hta + '@' + dbserver_hta + ':' + dbport_hta + '/' + dbname_hta)\n",
    "# conn3 = hta_engine.raw_connection()\n",
    "# cur3 = conn3.cursor()\n",
    "# sql = \"\"\"  SELECT adm0_name,iso3,map_code,map_done,url,shape as geometry\n",
    "#          FROM wfp_pub.wfp.wld_grid_100ka1_wfp where map_done = 'yes'  \"\"\"\n",
    "# remote_df = gpd.GeoDataFrame.from_postgis(sql,conn3,geom_col='geometry')\n",
    "# conn3.close()\n",
    "\n",
    "# # import folium\n",
    "# # m = folium.Map(tiles='cartodbdark_matter') #cartodbpositron #stamentoner #cartodbdark_matter\n",
    "# # folium.GeoJson(remote_df).add_to(m)\n",
    "# # m.fit_bounds(m.get_bounds())\n",
    "# # m\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# conn3 = hta_engine.raw_connection()\n",
    "# cur3 = conn3.cursor()\n",
    "# hta_sql = \"\"\"  SELECT adm0_name,iso3,map_code,map_done,url,shape as geometry\n",
    "#           FROM wfp_pub.wfp.wld_grid_100ka1_wfp where map_done != 'yes'  \"\"\"\n",
    "# remote_hta_gdf = gpd.GeoDataFrame.from_postgis(hta_sql,conn3,geom_col='geometry')\n",
    "# request_hta = gpd.overlay(remote_hta_gdf, alert, how='intersection')\n",
    "# #conn3.close()\n",
    "\n",
    "# config.read(os.path.join(root, \"config.txt\"))\n",
    "# dbserver_hta = config.get('hta','dbserver_hta')\n",
    "# dbname_hta = config.get('hta','dbname_hta')\n",
    "# dbuser_hta = config.get('hta','dbuser_hta')\n",
    "# dbpword_hta = config.get('hta','dbpword_hta')\n",
    "# dbport_hta = config.get('hta','dbport_hta')\n",
    "# hta_engine = create_engine('postgresql+psycopg2://' + dbuser_hta + ':' + dbpword_hta + '@' + dbserver_hta + ':' + dbport_hta + '/' + dbname_hta)\n",
    "# conn3 = hta_engine.raw_connection()\n",
    "# cur3 = conn3.cursor()\n",
    "# sql = \"\"\"  SELECT adm0_name,iso3,map_code,map_done,url,shape as geometry\n",
    "#          FROM wfp_pub.wfp.wld_grid_100ka1_wfp where map_done = 'yes'  \"\"\"\n",
    "# remote_df = gpd.GeoDataFrame.from_postgis(sql,conn3,geom_col='geometry')\n",
    "# conn3.close()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# ## Optional Plot ECMWF raster forecast\n",
    "# raster = rasterio.open(ecmwf_F_3D)\n",
    "# array = raster.read()\n",
    "# from IPython.display import Image\n",
    "# %matplotlib notebook\n",
    "# %matplotlib inline\n",
    "# show(array)\n",
    "\n",
    "\n",
    "# ### ECMWF Local Compute Processing - Work In Progress (xarray/dask)\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# out_file = '/Users/michael/GEO/process_cdsapi/'\n",
    "\n",
    "# c = cdsapi.Client()\n",
    "\n",
    "# c.retrieve(\n",
    "#     'reanalysis-era5-single-levels-monthly-means',\n",
    "#     {\n",
    "#         'format':'netcdf',\n",
    "#         'product_type':'monthly_averaged_reanalysis',\n",
    "#         'variable':'total_precipitation',\n",
    "#         'year':[\n",
    "#             '1979','1980','1981',\n",
    "#             '1982','1983','1984',\n",
    "#             '1985','1986','1987',\n",
    "#             '1988','1989','1990',\n",
    "#             '1991','1992','1993',\n",
    "#             '1994','1995','1996',\n",
    "#             '1997','1998','1999',\n",
    "#             '2000','2001','2002',\n",
    "#             '2003','2004','2005',\n",
    "#             '2006','2007','2008',\n",
    "#             '2009','2010','2011',\n",
    "#             '2012','2013','2014',\n",
    "#             '2015','2016','2017',\n",
    "#             '2018'\n",
    "#         ],\n",
    "#         'month':[\n",
    "#             '01','02','03',\n",
    "#             '04','05','06',\n",
    "#             '07','08','09',\n",
    "#             '10','11','12'\n",
    "#         ],\n",
    "#         'time':'00:00'\n",
    "#     },\n",
    "#     outfile + 'monthly_average_reanalysis.nc')\n",
    "\n",
    "# tot_precip = '/Users/michael/GEO/process_cdsapi/total_precip.tif'\n",
    "# cdsapi_stats = zonal_stats(alert, tot_precip, prefix='cds_x',geojson_out=True)\n",
    "# zonal_ppt_gdf = GeoDataFrame.from_features(cdsapi_stats)\n",
    "\n",
    "### Reference: https://github.com/jwagemann/seasonal_forecasts/blob/master/Workflow_seasonal_fc_processing.ipynb\n",
    "### Reference: https://annefou.github.io/metos_python/07-LargeFiles/\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#WorldClim V1\n",
    "#worldclim = ee.ImageCollection('WORLDCLIM/V1/MONTHLY')\n",
    "#precip = worldclim.select('prec')\n",
    "#collectionList = precip.toList(precip.size())\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#import hxl \n",
    "#source = hxl.data('http://wfp.org/dataset.xlsx')\n",
    "#from hdx.location.country import Country\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
